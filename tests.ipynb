{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class EspOptimizer(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr):\n",
    "        super().__init__(params, defaults={'lr': lr})\n",
    "\n",
    "    def step(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        # RNN Layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Initializing hidden state for first input using method defined below\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # This method generates the first hidden state of zeros which we'll use in the forward pass\n",
    "        # We'll send the tensor holding the hidden state to the device we specified earlier as well\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim)\n",
    "        return hidden"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "net = Net(n_input=2).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 5.9152e-01, -2.9711e-01],\n",
      "        [-5.1869e-01,  6.9431e-01],\n",
      "        [ 4.1480e-01,  5.4345e-01],\n",
      "        [-3.3186e-01, -4.4981e-01],\n",
      "        [-3.1292e-02, -5.9555e-01],\n",
      "        [-5.7582e-01,  2.6707e-01],\n",
      "        [ 5.6602e-02,  6.4653e-01],\n",
      "        [ 6.1133e-01, -3.3942e-01],\n",
      "        [-9.8532e-02,  1.9221e-01],\n",
      "        [-2.9972e-02, -6.9953e-01],\n",
      "        [ 5.6790e-01,  2.1745e-01],\n",
      "        [ 4.7219e-01,  2.8899e-01],\n",
      "        [ 2.9043e-01, -6.8306e-01],\n",
      "        [-6.8395e-01, -2.6371e-01],\n",
      "        [-7.4724e-02, -4.3051e-01],\n",
      "        [ 2.2411e-01, -3.6682e-01],\n",
      "        [-1.7248e-01,  1.5230e-02],\n",
      "        [ 5.4999e-01, -3.2879e-01],\n",
      "        [ 7.8122e-02, -6.5292e-01],\n",
      "        [-4.3029e-01,  2.0722e-01],\n",
      "        [ 6.9959e-02,  4.2203e-01],\n",
      "        [-6.9561e-01,  5.3183e-01],\n",
      "        [-1.3878e-02,  2.7656e-01],\n",
      "        [-5.9742e-01, -2.7688e-01],\n",
      "        [-1.5536e-01, -4.1506e-01],\n",
      "        [ 1.3531e-01,  4.7819e-01],\n",
      "        [ 1.9502e-01,  3.7489e-01],\n",
      "        [ 6.0417e-01, -1.4348e-01],\n",
      "        [-2.1499e-01, -4.5479e-01],\n",
      "        [ 7.6380e-02, -4.2522e-01],\n",
      "        [-1.3963e-01,  3.8702e-01],\n",
      "        [-4.3489e-01, -1.1994e-01],\n",
      "        [ 4.2006e-01, -5.5073e-01],\n",
      "        [ 4.8029e-01,  5.8097e-01],\n",
      "        [ 5.2452e-06,  1.6567e-01],\n",
      "        [-6.6544e-01,  4.5115e-01],\n",
      "        [-4.0177e-02, -1.5708e-02],\n",
      "        [ 4.9879e-01,  4.9929e-01],\n",
      "        [ 1.0997e-01, -3.4091e-01],\n",
      "        [-1.4311e-01,  3.1587e-01],\n",
      "        [ 2.8698e-02,  5.5482e-01],\n",
      "        [-3.4291e-01, -3.9055e-01],\n",
      "        [ 4.2812e-01, -1.8260e-01],\n",
      "        [ 3.5538e-01,  5.4349e-02],\n",
      "        [-8.7358e-02,  6.6094e-01],\n",
      "        [ 8.4381e-02, -4.2958e-01],\n",
      "        [ 4.6690e-01,  6.7389e-02],\n",
      "        [-1.6302e-01,  6.5300e-01],\n",
      "        [-3.4232e-01, -1.3758e-01],\n",
      "        [ 7.0190e-01, -5.8383e-01],\n",
      "        [ 5.3058e-01,  1.4378e-01],\n",
      "        [ 2.4170e-01, -6.2203e-01],\n",
      "        [ 4.7217e-01,  6.5928e-01],\n",
      "        [ 1.4404e-01,  2.1979e-01],\n",
      "        [ 6.2316e-01, -2.0002e-01],\n",
      "        [-5.7802e-01, -5.3751e-01],\n",
      "        [-2.2487e-01, -3.2625e-01],\n",
      "        [-6.9723e-01,  6.8465e-01],\n",
      "        [ 5.7168e-01, -3.7922e-01],\n",
      "        [ 5.9471e-01, -5.1928e-01],\n",
      "        [ 4.0281e-01,  6.4406e-02],\n",
      "        [-2.2199e-01,  4.8425e-01],\n",
      "        [-6.8012e-01,  9.8766e-02],\n",
      "        [-4.4186e-01, -6.9477e-01],\n",
      "        [ 6.0634e-01,  6.8861e-01],\n",
      "        [ 3.1736e-01,  6.3272e-02],\n",
      "        [-5.6967e-01, -7.9171e-02],\n",
      "        [-2.6326e-01,  3.7794e-01],\n",
      "        [-5.3339e-02, -6.9995e-01],\n",
      "        [ 5.8984e-01,  6.0179e-01],\n",
      "        [ 1.8630e-01,  5.9027e-01],\n",
      "        [ 1.6070e-01,  1.3108e-01],\n",
      "        [ 3.4446e-01, -1.0315e-02],\n",
      "        [ 5.8260e-01,  5.2361e-01],\n",
      "        [-2.0865e-01,  5.8920e-01],\n",
      "        [ 2.1816e-02, -3.4092e-01],\n",
      "        [ 2.9669e-01, -7.8952e-03],\n",
      "        [-1.8135e-01,  4.0742e-01],\n",
      "        [-2.8521e-01,  3.4190e-01],\n",
      "        [-1.6195e-01,  6.4887e-01],\n",
      "        [-6.8900e-01, -3.2813e-01],\n",
      "        [ 5.4112e-01, -1.3762e-01],\n",
      "        [-5.2702e-01,  5.8843e-01],\n",
      "        [-1.9178e-01, -3.9217e-01],\n",
      "        [ 1.1026e-01,  4.7978e-01],\n",
      "        [-6.9832e-01, -5.9024e-01],\n",
      "        [-1.8357e-01,  4.0304e-02],\n",
      "        [-5.5309e-01, -3.1004e-01],\n",
      "        [ 5.8160e-01,  5.4179e-01],\n",
      "        [-3.8134e-01, -2.1787e-01],\n",
      "        [-6.5673e-01, -3.8620e-01],\n",
      "        [ 4.9923e-01, -2.0785e-02],\n",
      "        [-2.0651e-01,  6.9248e-01],\n",
      "        [-2.9567e-01, -9.4927e-02],\n",
      "        [ 5.3365e-01,  4.1444e-01],\n",
      "        [-1.0861e-01,  3.4146e-01],\n",
      "        [ 1.5363e-01, -5.1955e-01],\n",
      "        [-6.8833e-01,  6.9627e-01],\n",
      "        [ 7.9907e-02,  3.1767e-01],\n",
      "        [ 1.2050e-01, -6.1519e-01],\n",
      "        [-5.6161e-01, -1.4520e-01],\n",
      "        [ 2.2962e-01, -4.1049e-01],\n",
      "        [-4.6783e-01,  3.3546e-02],\n",
      "        [-2.2864e-01, -5.8529e-01],\n",
      "        [-2.4622e-02,  5.6667e-02],\n",
      "        [ 4.1204e-02,  3.6925e-01],\n",
      "        [-5.0815e-01,  1.9022e-01],\n",
      "        [ 3.1547e-01, -5.7625e-01],\n",
      "        [ 4.1924e-01, -6.7481e-01],\n",
      "        [ 4.2747e-01, -6.1328e-01],\n",
      "        [ 5.0195e-01, -3.3259e-02],\n",
      "        [-5.9703e-01, -2.7091e-01],\n",
      "        [ 4.4123e-01,  7.0365e-01],\n",
      "        [-3.4361e-01, -4.7098e-01],\n",
      "        [ 1.9927e-01,  6.3350e-01],\n",
      "        [-4.4815e-01, -3.1326e-01],\n",
      "        [ 3.3919e-01,  3.9622e-01],\n",
      "        [ 1.7926e-01,  7.3766e-03],\n",
      "        [-6.9440e-01, -6.8557e-01],\n",
      "        [ 5.0757e-01, -1.0537e-01],\n",
      "        [ 6.9182e-01, -3.3138e-01],\n",
      "        [ 1.4037e-01,  4.9947e-01],\n",
      "        [-4.6269e-01, -1.8440e-01],\n",
      "        [ 1.2541e-02, -1.2464e-01],\n",
      "        [ 6.3230e-01, -3.3604e-01],\n",
      "        [-5.8293e-01, -2.8550e-01],\n",
      "        [ 7.0106e-01, -2.9140e-01],\n",
      "        [ 5.9240e-01,  2.8802e-01],\n",
      "        [-5.2765e-01,  5.6552e-01],\n",
      "        [ 4.0845e-01,  4.9273e-01],\n",
      "        [ 3.9241e-01, -5.6512e-01],\n",
      "        [-4.1550e-01,  2.6392e-03],\n",
      "        [-2.3209e-01,  3.3479e-01],\n",
      "        [ 6.8926e-01, -3.7624e-01],\n",
      "        [ 5.3588e-01,  3.6163e-01],\n",
      "        [-5.2628e-01, -4.5771e-01],\n",
      "        [-5.5125e-01, -5.5193e-01],\n",
      "        [ 4.5333e-01, -6.3059e-01],\n",
      "        [-6.0425e-01, -2.0615e-01],\n",
      "        [ 6.7184e-01, -5.5100e-01],\n",
      "        [ 6.8639e-01, -1.0666e-01],\n",
      "        [ 5.9175e-02,  6.5317e-02],\n",
      "        [-1.9029e-01,  5.3383e-01],\n",
      "        [-1.5722e-01, -1.9245e-01],\n",
      "        [ 5.0406e-01,  2.8782e-01],\n",
      "        [ 1.7785e-01,  6.2085e-01],\n",
      "        [-2.1372e-01, -5.0494e-01],\n",
      "        [ 2.0493e-01, -3.2442e-01],\n",
      "        [ 6.6088e-01, -2.4175e-02],\n",
      "        [ 6.0261e-01, -5.6714e-01],\n",
      "        [ 2.9911e-01,  5.3228e-01],\n",
      "        [-6.9044e-01,  5.5377e-01],\n",
      "        [-4.7389e-01, -3.8364e-01],\n",
      "        [-3.7860e-01,  5.3220e-01],\n",
      "        [ 1.3423e-01, -5.9537e-01],\n",
      "        [-2.6058e-01,  5.4958e-01],\n",
      "        [ 9.7576e-02, -1.4139e-01],\n",
      "        [ 3.9974e-01, -5.5221e-01],\n",
      "        [-3.9903e-01, -3.9116e-02],\n",
      "        [-4.3778e-01, -5.9269e-01],\n",
      "        [-4.1068e-01, -3.0450e-01],\n",
      "        [-4.6472e-01, -2.5727e-02],\n",
      "        [-4.1928e-01,  2.8059e-01],\n",
      "        [-9.5005e-02, -3.8217e-01],\n",
      "        [-4.4588e-01,  4.4181e-01],\n",
      "        [-5.7496e-01,  4.3247e-02],\n",
      "        [-2.9538e-01, -5.8316e-01],\n",
      "        [-2.8414e-01,  4.2029e-02],\n",
      "        [-2.3581e-01, -2.2289e-01],\n",
      "        [-1.2518e-01,  5.3879e-01],\n",
      "        [ 2.6342e-01, -9.3824e-02],\n",
      "        [ 2.8499e-01,  4.6831e-01],\n",
      "        [ 4.6551e-01, -5.3338e-01],\n",
      "        [-2.0359e-01,  6.7049e-01],\n",
      "        [-4.3305e-01, -5.4091e-01],\n",
      "        [-2.1924e-01,  1.9011e-01],\n",
      "        [-1.3402e-01, -2.0999e-01],\n",
      "        [-1.8218e-01, -3.8070e-02],\n",
      "        [-7.9757e-02, -1.0692e-01],\n",
      "        [ 8.4076e-02,  6.2148e-01],\n",
      "        [ 4.5958e-01, -2.3895e-01],\n",
      "        [ 4.4987e-01,  2.2258e-01],\n",
      "        [-4.1643e-01, -2.5113e-01],\n",
      "        [ 1.8850e-01, -2.3913e-01],\n",
      "        [ 6.1557e-01,  2.2389e-01],\n",
      "        [-2.7575e-01,  5.0266e-01],\n",
      "        [-5.4986e-01, -2.6147e-01],\n",
      "        [-2.9493e-01,  6.6128e-01],\n",
      "        [-4.2116e-01, -5.8777e-01],\n",
      "        [-6.3741e-02, -5.1479e-01],\n",
      "        [ 3.8972e-01,  2.6974e-01],\n",
      "        [ 2.9741e-01, -5.6926e-01],\n",
      "        [-4.9388e-01, -2.3438e-01],\n",
      "        [-4.8550e-02, -1.5470e-01],\n",
      "        [ 1.9720e-01, -4.0999e-01],\n",
      "        [-6.5782e-01, -5.1871e-01],\n",
      "        [ 5.1842e-02, -6.0979e-02],\n",
      "        [-1.2318e-02,  5.5971e-01],\n",
      "        [-4.5150e-01,  6.2072e-01],\n",
      "        [ 1.9415e-01, -2.8234e-02],\n",
      "        [-5.9803e-01, -4.5156e-01],\n",
      "        [-5.4626e-01, -4.4494e-01],\n",
      "        [-2.4008e-01, -3.4954e-02],\n",
      "        [-5.2080e-01, -5.1931e-01],\n",
      "        [-7.0161e-01,  4.6908e-01],\n",
      "        [ 4.6850e-01,  5.3017e-01],\n",
      "        [ 5.2320e-01,  3.6483e-01],\n",
      "        [ 3.7216e-01,  4.5415e-01],\n",
      "        [-6.2517e-01, -1.7373e-01],\n",
      "        [ 6.3875e-01, -4.4726e-01],\n",
      "        [ 8.5781e-02, -6.9341e-01],\n",
      "        [ 5.2314e-01,  2.5968e-01],\n",
      "        [ 7.0334e-01,  2.8476e-01],\n",
      "        [-6.7910e-01,  2.8211e-01],\n",
      "        [-4.7830e-01,  5.6424e-01],\n",
      "        [-6.8059e-01,  5.7969e-01],\n",
      "        [ 1.7245e-01,  3.4197e-01],\n",
      "        [-1.2261e-01, -1.0572e-01],\n",
      "        [ 7.9267e-02, -3.3773e-01],\n",
      "        [ 5.8523e-01,  2.3052e-01],\n",
      "        [-2.0033e-01, -4.7911e-01],\n",
      "        [-5.3440e-01,  2.6449e-01],\n",
      "        [-1.6682e-03, -4.2681e-01],\n",
      "        [-1.7732e-01,  1.1219e-01],\n",
      "        [-6.7344e-01,  1.3779e-01],\n",
      "        [ 2.5172e-01,  2.0545e-01],\n",
      "        [ 4.9578e-01, -3.2570e-01],\n",
      "        [ 6.2305e-01,  6.5453e-01],\n",
      "        [ 5.7598e-01,  6.4324e-01],\n",
      "        [-5.3711e-01, -4.3917e-01],\n",
      "        [-7.0586e-01, -1.0512e-01],\n",
      "        [ 4.5917e-01, -6.4935e-01],\n",
      "        [ 4.5331e-01, -5.9837e-01],\n",
      "        [-5.4234e-01, -3.4849e-01],\n",
      "        [ 6.5141e-01,  3.8464e-01],\n",
      "        [-2.2600e-01, -3.2816e-01],\n",
      "        [ 5.4731e-01, -6.5174e-01],\n",
      "        [ 1.1304e-01,  3.3096e-02],\n",
      "        [ 2.0292e-01,  3.4865e-03],\n",
      "        [-2.8423e-01, -1.6348e-01],\n",
      "        [-1.0387e-01, -6.0164e-01],\n",
      "        [ 7.8918e-02,  1.6099e-01],\n",
      "        [-1.5366e-01, -5.7137e-02],\n",
      "        [ 5.6888e-01, -4.0146e-02],\n",
      "        [-5.7582e-01, -3.2769e-01],\n",
      "        [ 3.0690e-01,  2.7351e-01],\n",
      "        [ 5.7713e-01, -2.7114e-01],\n",
      "        [-7.0576e-01,  3.1312e-01],\n",
      "        [ 6.9903e-01, -6.4459e-02],\n",
      "        [-2.3817e-02,  6.2714e-01],\n",
      "        [ 6.7615e-01,  9.1722e-02],\n",
      "        [-3.4344e-01,  5.6807e-01],\n",
      "        [-6.6585e-01, -1.7593e-01],\n",
      "        [ 5.9009e-01,  3.2437e-01],\n",
      "        [ 1.4623e-01, -1.5244e-01],\n",
      "        [ 3.3949e-01,  4.8587e-01]], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for parameter in net.parameters():\n",
    "    print(parameter)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "loss = nn.BCELoss()\n",
    "global_accuracy_train = []\n",
    "global_accuracy_val = []\n",
    "for i in range(150):\n",
    "    accuracy_train = []\n",
    "    train_gen = generate_batch(X_train, y_train, batch_size=256)\n",
    "    val_gen = generate_batch(X_test, y_test, batch_size=256)\n",
    "    for x, y in train_gen:\n",
    "        x_tensor = torch.from_numpy(x).float().to(device)\n",
    "        y_tensor = torch.from_numpy(y.values).double().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = net.forward(x_tensor)\n",
    "        loss_value = loss(y_pred.view(-1).double(), y_tensor)\n",
    "        accuracy_train.append(accuracy_score(y_pred.view(-1).long().detach().cpu().numpy(), y.values))\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    accuracy_val = []\n",
    "    for x, y in val_gen:\n",
    "        with torch.no_grad():\n",
    "            x_tensor = torch.from_numpy(x).float().to(device)\n",
    "            y_pred = net.forward(x_tensor).to(device)\n",
    "            accuracy_val.append(accuracy_score(y_pred.view(-1).long().detach().cpu().numpy(), y))\n",
    "    if i % 5 == 0:\n",
    "        print(f'Эпоха {i}, Точность: На обучающей: {np.around(np.mean(accuracy_train)*100, 4)}%, на валидационной: {np.around(np.mean(accuracy_val)*100, 4)}%')\n",
    "    global_accuracy_train.append(np.mean(accuracy_train))\n",
    "    global_accuracy_val.append(np.mean(accuracy_val))\n",
    "\n",
    "x_tensor = torch.from_numpy(X_test).float().to(device)\n",
    "y_pred = net.forward(x_tensor).to(device)\n",
    "acc = accuracy_score(y_pred.view(-1).long().detach().cpu().numpy(), y_test)\n",
    "print(f'Точность на тестовой: {np.around(acc*100, 4)}%')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
